{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c93aecb-14c7-41a1-ae3b-0c11927d199c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JAVA_HOME configurado correctamente\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "print(\"✅ JAVA_HOME configurado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24b2cfc5-7a96-4bdf-906f-34672a057e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark funcionando con Java 17\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VertexDataproc1\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark funcionando con Java 17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e026070-f732-46b9-ae23-21a0a86a4314",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/30 00:50:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Cambia estos valores a los tuyos\n",
    "project_id = \"sesion04-461702\"\n",
    "region = \"us-central1\"\n",
    "cluster_name = \"pyspark-cluster\"\n",
    "\n",
    "# Crea la sesión Spark que se conecta a Dataproc\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VertexDataproc1\") \\\n",
    "    .master(f\"yarn\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.yarn.cluster.resource\", f\"{region}/{cluster_name}\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Prueba PySpark\n",
    "spark.range(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ffd4cc7-d0b4-4bcc-9aae-d8367bd49f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/30 01:52:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|Google|\n",
      "|  2| Cloud|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mi_script.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataprocJob\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([(1, 'Google'), (2, 'Cloud')], [\"id\", \"name\"])\n",
    "df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7684d1f1-a8d5-468d-8207-556ee8dfb075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [6b80b748aa3547b1b823f0c1ddcf6609] submitted.\n",
      "Waiting for job output...\n",
      "25/07/30 01:53:51 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/07/30 01:53:51 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/07/30 01:53:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/07/30 01:53:51 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/07/30 01:53:52 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "25/07/30 01:53:52 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "25/07/30 01:53:52 INFO MetricsSystemImpl: google-hadoop-file-system metrics system started\n",
      "25/07/30 01:53:52 INFO DataprocSparkPlugin: Registered 188 driver metrics\n",
      "25/07/30 01:53:53 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at pyspark-cluster-m.us-central1-c.c.sesion04-461702.internal./10.128.0.9:8032\n",
      "25/07/30 01:53:53 INFO AHSProxy: Connecting to Application History server at pyspark-cluster-m.us-central1-c.c.sesion04-461702.internal./10.128.0.9:10200\n",
      "25/07/30 01:53:53 INFO Configuration: resource-types.xml not found\n",
      "25/07/30 01:53:53 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "25/07/30 01:53:54 INFO YarnClientImpl: Submitted application application_1753829199976_0003\n",
      "25/07/30 01:53:55 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at pyspark-cluster-m.us-central1-c.c.sesion04-461702.internal./10.128.0.9:8030\n",
      "25/07/30 01:53:57 INFO GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "25/07/30 01:53:58 INFO GoogleHadoopOutputStream: hflush(): No-op due to rate limit (RateLimiter[stableRate=0.2qps]): readers will *not* yet see flushed data for gs://dataproc-temp-us-central1-390075199183-wawihcg8/ef0fa366-0406-4da1-8fe0-8cc945fd0191/spark-job-history/application_1753829199976_0003.inprogress [CONTEXT ratelimit_period=\"1 MINUTES\" ]\n",
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|Google|\n",
      "|  2| Cloud|\n",
      "+---+------+\n",
      "\n",
      "25/07/30 01:54:07 INFO DataprocSparkPlugin: Shutting down driver plugin. metrics=[action_http_patch_request=0, files_created=1, gcs_api_server_timeout_count=0, op_get_list_status_result_size=0, op_open=0, action_http_delete_request=1, gcs_api_time=1522, gcs_backoff_count=0, gcs_api_client_unauthorized_response_count=0, stream_read_close_operations=0, stream_read_bytes_backwards_on_seek=0, gs_filesystem_create=3, exception_count=0, gcs_exception_count=0, gcs_api_total_request_count=25, op_create=1, stream_read_vectored_operations=0, gcs_metadata_request=14, gcs_api_client_bad_request_count=0, action_http_put_request=2, op_create_non_recursive=0, gcs_api_client_gone_response_count=0, gs_filesystem_initialize=2, stream_read_vectored_incoming_ranges=0, stream_write_operations=0, gcs_list_dir_request=0, stream_read_operations=0, gcs_api_client_request_timeout_count=0, op_rename=0, op_get_file_status=1, op_glob_status=0, op_exists=0, stream_write_bytes=159894, op_xattr_list=0, op_get_delegation_token=0, gcs_api_server_unavailable_count=0, directories_created=1, files_delete_rejected=0, stream_read_vectored_combined_ranges=0, op_xattr_get_named=0, gcs_list_file_request=2, op_hsync=0, action_http_get_request=0, stream_read_operations_incomplete=0, op_delete=0, stream_read_bytes=0, gcs_api_client_non_found_response_count=12, op_list_located_status=0, gcs_api_client_requested_range_not_statisfiable_count=0, op_hflush=18, op_list_status=0, stream_read_vectored_read_bytes_discarded=0, op_xattr_get_named_map=0, gcs_api_client_side_error_count=13, op_get_file_checksum=0, gcs_api_server_internal_error_count=0, stream_read_seek_bytes_skipped=0, stream_write_close_operations=0, gcs_get_media_request=0, gcs_connector_time=1464, files_deleted=0, action_http_post_request=5, op_mkdirs=1, gcs_api_client_rate_limit_error_count=0, op_copy_from_local_file=0, gcs_api_server_bad_gateway_count=0, stream_readVectored_range_duration=0, stream_read_seek_backward_operations=0, gcs_api_server_side_error_count=0, stream_read_seek_operations=0, gcs_get_other_request=1, stream_read_seek_forward_operations=0, gcs_api_client_precondition_failed_response_count=1, op_xattr_get_map=0, delegation_tokens_issued=0, gcs_backoff_time=0, gcs_list_dir_request_min=0, gcs_metadata_request_min=21, op_delete_min=0, op_glob_status_min=0, op_create_non_recursive_min=0, op_hsync_min=0, op_xattr_get_named_min=0, op_xattr_get_named_map_min=0, op_hflush_min=0, op_xattr_list_min=0, action_http_put_request_min=104, op_open_min=0, gcs_list_file_request_min=34, stream_write_close_operations_min=0, op_create_min=87, action_http_delete_request_min=50, op_mkdirs_min=199, op_list_status_min=0, gcs_get_media_request_min=0, stream_readVectored_range_duration_min=0, stream_read_vectored_operations_min=0, stream_read_close_operations_min=0, stream_read_operations_min=0, stream_read_seek_operations_min=0, op_xattr_get_map_min=0, stream_write_operations_min=0, action_http_patch_request_min=0, op_get_file_status_min=524, op_rename_min=0, delegation_tokens_issued_min=0, action_http_post_request_min=40, stream_read_close_operations_max=0, stream_read_seek_operations_max=0, op_hflush_max=427, op_xattr_list_max=0, op_xattr_get_map_max=0, action_http_put_request_max=145, action_http_patch_request_max=0, action_http_post_request_max=118, stream_write_close_operations_max=0, action_http_delete_request_max=50, op_mkdirs_max=199, gcs_get_media_request_max=0, op_rename_max=0, stream_read_vectored_operations_max=0, stream_readVectored_range_duration_max=0, op_xattr_get_named_map_max=0, stream_write_operations_max=0, stream_read_operations_max=0, op_xattr_get_named_max=0, op_glob_status_max=0, op_create_non_recursive_max=0, op_get_file_status_max=524, op_open_max=0, delegation_tokens_issued_max=0, gcs_list_file_request_max=186, gcs_metadata_request_max=239, op_create_max=87, op_delete_max=0, op_list_status_max=0, op_hsync_max=0, gcs_list_dir_request_max=0, op_open_mean=0, op_xattr_list_mean=0, op_rename_mean=0, op_xattr_get_map_mean=0, gcs_list_dir_request_mean=0, op_glob_status_mean=0, stream_read_seek_operations_mean=0, gcs_list_file_request_mean=110, stream_write_operations_mean=0, op_hflush_mean=36, gcs_metadata_request_mean=45, op_list_status_mean=0, stream_read_close_operations_mean=0, op_xattr_get_named_map_mean=0, stream_read_vectored_operations_mean=0, op_mkdirs_mean=199, action_http_post_request_mean=65, stream_write_close_operations_mean=0, action_http_put_request_mean=124, action_http_patch_request_mean=0, op_hsync_mean=0, delegation_tokens_issued_mean=0, action_http_delete_request_mean=50, stream_read_operations_mean=0, op_create_mean=87, op_delete_mean=0, op_create_non_recursive_mean=0, stream_readVectored_range_duration_mean=0, op_xattr_get_named_mean=0, gcs_get_media_request_mean=0, op_get_file_status_mean=524, op_delete_duration=0, op_get_file_status_duration=524, action_http_put_request_duration=249, stream_write_operations_duration=0, op_hsync_duration=0, gcs_metadata_request_duration=632, gcs_get_media_request_duration=0, gcs_list_file_request_duration=220, op_list_status_duration=0, op_mkdirs_duration=199, op_open_duration=0, op_create_duration=87, op_hflush_duration=654, gcs_list_dir_request_duration=0, op_glob_status_duration=0, stream_read_operations_duration=0, action_http_delete_request_duration=50, action_http_post_request_duration=325, op_rename_duration=0]\n",
      "Job [6b80b748aa3547b1b823f0c1ddcf6609] finished successfully.\n",
      "done: true\n",
      "driverControlFilesUri: gs://dataproc-staging-us-central1-390075199183-0yr5fclb/google-cloud-dataproc-metainfo/ef0fa366-0406-4da1-8fe0-8cc945fd0191/jobs/6b80b748aa3547b1b823f0c1ddcf6609/\n",
      "driverOutputResourceUri: gs://dataproc-staging-us-central1-390075199183-0yr5fclb/google-cloud-dataproc-metainfo/ef0fa366-0406-4da1-8fe0-8cc945fd0191/jobs/6b80b748aa3547b1b823f0c1ddcf6609/driveroutput\n",
      "jobUuid: fc3fd55b-c627-38c3-8d97-6a8d3c193d35\n",
      "placement:\n",
      "  clusterName: pyspark-cluster\n",
      "  clusterUuid: ef0fa366-0406-4da1-8fe0-8cc945fd0191\n",
      "pysparkJob:\n",
      "  mainPythonFileUri: gs://dataproc-staging-us-central1-390075199183-0yr5fclb/google-cloud-dataproc-metainfo/ef0fa366-0406-4da1-8fe0-8cc945fd0191/jobs/6b80b748aa3547b1b823f0c1ddcf6609/staging/Dataproc-pyspark.py\n",
      "reference:\n",
      "  jobId: 6b80b748aa3547b1b823f0c1ddcf6609\n",
      "  projectId: sesion04-461702\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2025-07-30T01:54:09.134416Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2025-07-30T01:53:46.163343Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2025-07-30T01:53:46.194427Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2025-07-30T01:53:46.442844Z'\n",
      "yarnApplications:\n",
      "- name: DataprocJob\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://pyspark-cluster-m.us-central1-c.c.sesion04-461702.internal.:8088/proxy/application_1753829199976_0003/\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit pyspark Dataproc-pyspark.py   --cluster=pyspark-cluster   --region=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84d07b-ccd6-4544-ab26-da089aee3d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
