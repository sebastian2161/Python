{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c93aecb-14c7-41a1-ae3b-0c11927d199c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JAVA_HOME configurado correctamente\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "print(\"✅ JAVA_HOME configurado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24b2cfc5-7a96-4bdf-906f-34672a057e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark funcionando con Java 17\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VertexDataproc1\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark funcionando con Java 17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e026070-f732-46b9-ae23-21a0a86a4314",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/30 00:50:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Cambia estos valores a los tuyos\n",
    "project_id = \"sesion04-461702\"\n",
    "region = \"us-central1\"\n",
    "cluster_name = \"pyspark-cluster\"\n",
    "\n",
    "# Crea la sesión Spark que se conecta a Dataproc\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VertexDataproc1\") \\\n",
    "    .master(f\"yarn\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.yarn.cluster.resource\", f\"{region}/{cluster_name}\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Prueba PySpark\n",
    "spark.range(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ffd4cc7-d0b4-4bcc-9aae-d8367bd49f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/30 01:52:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|Google|\n",
      "|  2| Cloud|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mi_script.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataprocJob\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([(1, 'Google'), (2, 'Cloud')], [\"id\", \"name\"])\n",
    "df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7684d1f1-a8d5-468d-8207-556ee8dfb075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [6b80b748aa3547b1b823f0c1ddcf6609] submitted.\n",
      "Waiting for job output...\n",
      "25/07/30 01:53:51 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/07/30 01:53:51 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/07/30 01:53:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/07/30 01:53:51 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/07/30 01:53:52 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "25/07/30 01:53:52 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "25/07/30 01:53:52 INFO MetricsSystemImpl: google-hadoop-file-system metrics system started\n",
      "25/07/30 01:53:52 INFO DataprocSparkPlugin: Registered 188 driver metrics\n",
      "25/07/30 01:53:53 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at pyspark-cluster-m.us-central1-c.c.sesion04-461702.internal./10.128.0.9:8032\n",
      "25/07/30 01:53:53 INFO AHSProxy: Connecting to Application History server at pyspark-cluster-m.us-central1-c.c.sesion04-461702.internal./10.128.0.9:10200\n",
      "25/07/30 01:53:53 INFO Configuration: resource-types.xml not found\n",
      "25/07/30 01:53:53 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "25/07/30 01:53:54 INFO YarnClientImpl: Submitted application application_1753829199976_0003\n",
      "25/07/30 01:53:55 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at pyspark-cluster-m.us-central1-c.c.sesion04-461702.internal./10.128.0.9:8030\n",
      "25/07/30 01:53:57 INFO GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "25/07/30 01:53:58 INFO GoogleHadoopOutputStream: hflush(): No-op due to rate limit (RateLimiter[stableRate=0.2qps]): readers will *not* yet see flushed data for gs://dataproc-temp-us-central1-390075199183-wawihcg8/ef0fa366-0406-4da1-8fe0-8cc945fd0191/spark-job-history/application_1753829199976_0003.inprogress [CONTEXT ratelimit_period=\"1 MINUTES\" ]\n",
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|Google|\n",
      "|  2| Cloud|\n",
      "+---+------+\n",
      "\n",
      "Job [6b80b748aa3547b1b823f0c1ddcf6609] finished successfully.\n",
      "done: true\n",
      "driverControlFilesUri: gs://dataproc-staging-us-central1-390075199183-0yr5fclb/google-cloud-dataproc-metainfo/ef0fa366-0406-4da1-8fe0-8cc945fd0191/jobs/6b80b748aa3547b1b823f0c1ddcf6609/\n",
      "driverOutputResourceUri: gs://dataproc-staging-us-central1-390075199183-0yr5fclb/google-cloud-dataproc-metainfo/ef0fa366-0406-4da1-8fe0-8cc945fd0191/jobs/6b80b748aa3547b1b823f0c1ddcf6609/driveroutput\n",
      "jobUuid: fc3fd55b-c627-38c3-8d97-6a8d3c193d35\n",
      "placement:\n",
      "  clusterName: pyspark-cluster\n",
      "  clusterUuid: ef0fa366-0406-4da1-8fe0-8cc945fd0191\n",
      "pysparkJob:\n",
      "  mainPythonFileUri: gs://dataproc-staging-us-central1-390075199183-0yr5fclb/google-cloud-dataproc-metainfo/ef0fa366-0406-4da1-8fe0-8cc945fd0191/jobs/6b80b748aa3547b1b823f0c1ddcf6609/staging/Dataproc-pyspark.py\n",
      "reference:\n",
      "  jobId: 6b80b748aa3547b1b823f0c1ddcf6609\n",
      "  projectId: sesion04-461702\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2025-07-30T01:54:09.134416Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2025-07-30T01:53:46.163343Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2025-07-30T01:53:46.194427Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2025-07-30T01:53:46.442844Z'\n",
      "yarnApplications:\n",
      "- name: DataprocJob\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://pyspark-cluster-m.us-central1-c.c.sesion04-461702.internal.:8088/proxy/application_1753829199976_0003/\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit pyspark Dataproc-pyspark.py   --cluster=pyspark-cluster   --region=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84d07b-ccd6-4544-ab26-da089aee3d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
